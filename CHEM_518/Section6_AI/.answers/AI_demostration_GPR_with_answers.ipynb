{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c68a346-31c6-4f6b-881f-c3448ca0e35a",
   "metadata": {},
   "source": [
    "This tutorial will introduce how to train a Gaussian Process Regression (GPR) model and evaulate its effectiveness for a materials or chemical property. GPR is a kernel based method that assumes each sample in a data set is drawn from a gaussian process (random variable) and uses that to generate a mean prediction across all space with an associated uncertainty. In essence what GPR is doing is fitting a basis-set expansion of a true function where the basis set is defined by the training data and a kernel, which define the location and shape of the basis functions, respectively. For small to medium sized datasets GPR is a powerful technique, but can become expensive for a larger number of data points (thousands) and features (hundreds).\n",
    "\n",
    "This tutorial will demonstrate what GPRs are doing on a toy 1D problem, and demonstrate how to actually use it for a real material system by modelling the volume of a cubic-perovskite unitcell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c982eb4-3469-4d40-a660-1ce7feddf284",
   "metadata": {},
   "source": [
    "To start we will generate a simple example data set to demonstrate how GPR actually models a function, by create a test function with numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0fba8-7d2f-4bcf-86e7-52d0e4b82c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1)\n",
    "y = np.squeeze(X * np.sin(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36c701e-074c-45cc-9c5d-1ed28da2515d",
   "metadata": {},
   "source": [
    "Now that the function exists lets visualize it using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc32eb2-8cc2-4768-8555-2468d1e68636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "plt.title(\"True generative process\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b5c86-2afb-4377-9b8e-49195cf88225",
   "metadata": {},
   "source": [
    "We can now randomly select some data points to act as a training set using a random number generator. \n",
    "\n",
    "These pseudo random number generators use a seed integer value to produce a series of numbers that follow a particular distribution to approximate randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d86e9f-f9da-4f90-8481-8da831f88a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42af25-036e-45dd-aa9b-6cf5a4f3a6b7",
   "metadata": {},
   "source": [
    "With this training set we can now train our first GPR model using scikit-learn, one of the more popular general-ML packages for python using the radial basis function (RBF) kernel. The RBF kernel is one of the more popular ones used in machine learning and is defined as a Gaussian distance matrix $K\\left(\\mathbf{x}, \\mathbf{x'}\\right)=\\exp\\left(\\frac{\\left|\\left| \\mathbf{x} - \\mathbf{x'} \\right|\\right|^2}{2 \\sigma^2}\\right),$ with a hyperparmeter of $\\sigma$ representing the length scale of the Gaussian. With scikit-learn we can set the bounds of the length scale and it will find the optimial value for training in that region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee97f22-0115-48ac-9a0d-7802a39ca483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "gaussian_process.kernel_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590cb6f7-f2d3-4091-80df-78e486215a21",
   "metadata": {},
   "source": [
    "We can now use this model to get an estimate of the true function across the defined domain. Because we are using the exact data and did not set a regularization value, the model goes through each of the points exactly with no uncertainty. While it might be tempting to think of uncertainty as an error metric it is important to remember that uncertainty and error represent two different concepts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd2d9e0-d312-4dfb-9165-33b4691df095",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d6f6d3-dcb4-49d8-99aa-62369c5c0e6a",
   "metadata": {},
   "source": [
    "**Problem 1**\n",
    "\n",
    "Select a different set of random points, retrain a GPR model, and describe what the differences are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc47268-8a04-445e-9d60-ad31cc28b111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For consistent results upon rerunning cells\n",
    "rng = np.random.RandomState(5)\n",
    "\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "\n",
    "kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))\n",
    "gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9)\n",
    "gaussian_process.fit(X_train, y_train)\n",
    "gaussian_process.kernel_\n",
    "\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.scatter(X_train, y_train, label=\"Observations\")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on noise-free dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee55e18-c12e-4c78-9045-4447a9981ca5",
   "metadata": {},
   "source": [
    "While the above models can accurately predict the function with a small number of data points, if they are somewhat evenly distributed, it was trained on perfect data (without noise or any errors). Unfortuately, in chemistry and materials science we almost never have perfect data. What happens if we add noise to the data and model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0255f49d-0087-4047-adf9-4ff4979b46d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "\n",
    "training_indices = rng.choice(np.arange(y.size), size=6, replace=False)\n",
    "X_train, y_train = X[training_indices], y[training_indices]\n",
    "\n",
    "# rng can also use other distributions than the standard uniform one used normally\n",
    "noise_std = 0.75\n",
    "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a794b-5cae-40f7-9e47-09240ddf60cc",
   "metadata": {},
   "source": [
    "We can now add a term to our GPR model to account for this additional uncertainty of our data set $\\alpha$ represents the intrinsic variance in our measurements and can be set to the square of the noise's standard devation. To account for this we can also restart the training procedure to better estimate the kernel's hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68a4f4-71c3-48ca-a932-052024343c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n",
    ")\n",
    "gaussian_process.fit(X_train, y_train_noisy)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "print(gaussian_process.kernel_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0ae2c0-f71f-4db3-8c8b-32b10e68e311",
   "metadata": {},
   "source": [
    "We can now plot the results and compare it to the previous exmples. Because of the $\\alpha$ parameter there is never a point where the uncertainty goes to zero, and overall the model is less confident of what the true function looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5631e-2d45-4663-93f1-dd61887aa80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.errorbar(\n",
    "    X_train,\n",
    "    y_train_noisy,\n",
    "    noise_std,\n",
    "    linestyle=\"None\",\n",
    "    color=\"tab:blue\",\n",
    "    marker=\".\",\n",
    "    markersize=10,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on a noisy dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3609c-ecb4-48e9-845a-1edf5d073b4b",
   "metadata": {},
   "source": [
    "**Problem 2**\n",
    "\n",
    "Incrase the std_noise to 1.5 and reduce it 0.375 and see how that affects the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e136f904-fb30-427d-94c5-6a4a5ff3207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = 1.5\n",
    "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\n",
    "\n",
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n",
    ")\n",
    "gaussian_process.fit(X_train, y_train_noisy)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "print(gaussian_process.kernel_)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.errorbar(\n",
    "    X_train,\n",
    "    y_train_noisy,\n",
    "    noise_std,\n",
    "    linestyle=\"None\",\n",
    "    color=\"tab:blue\",\n",
    "    marker=\".\",\n",
    "    markersize=10,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on a noisy dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90b8d9-a250-4fd6-81b9-702664041970",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_std = 0.375\n",
    "y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape)\n",
    "\n",
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9\n",
    ")\n",
    "gaussian_process.fit(X_train, y_train_noisy)\n",
    "mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)\n",
    "print(gaussian_process.kernel_)\n",
    "\n",
    "plt.plot(X, y, label=r\"$f(x) = x \\sin(x)$\", linestyle=\"dotted\")\n",
    "plt.errorbar(\n",
    "    X_train,\n",
    "    y_train_noisy,\n",
    "    noise_std,\n",
    "    linestyle=\"None\",\n",
    "    color=\"tab:blue\",\n",
    "    marker=\".\",\n",
    "    markersize=10,\n",
    "    label=\"Observations\",\n",
    ")\n",
    "plt.plot(X, mean_prediction, label=\"Mean prediction\")\n",
    "plt.fill_between(\n",
    "    X.ravel(),\n",
    "    mean_prediction - 1.96 * std_prediction,\n",
    "    mean_prediction + 1.96 * std_prediction,\n",
    "    color=\"tab:orange\",\n",
    "    alpha=0.5,\n",
    "    label=r\"95% confidence interval\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"$f(x)$\")\n",
    "_ = plt.title(\"Gaussian process regression on a noisy dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdff9bd-b38b-4950-a8d6-4c1a07fc9faf",
   "metadata": {},
   "source": [
    "Now that we have an understanding of what GPR is doing, now lets apply it to a real-world problem for predicting the volume of a cubic-perovskite unit cell volume. \n",
    "\n",
    "For this problem we will take a data set of preovskite's volumes and the average value for a series of average elemental properties of the given atoms inside of that unit cell taken from online databases, you can see more information about this here: [Journal of Materials Research 38 (20), 4477-4496](https://link.springer.com/article/10.1557/s43578-023-01164-w)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f339440-9009-4dce-8e51-29a56ebe6c2d",
   "metadata": {},
   "source": [
    "To get the data we are using the pandas DataFrame and reading from a CSV file. In the ML community and statistics in general pandas is one of the most widely used packages with support for numpy, matplotlib, and scikit-learn. Here we are going to pull out the volume information and use that as our y-points or our target property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ac56c8-5fd9-4fd8-906d-c0f195ca49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"perovskite_data.csv\", index_col=0)\n",
    "\n",
    "volume = df[\"Volume\"]\n",
    "df.drop(columns=[\"Volume\"], inplace=True)\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6cfc76-3136-4de9-9863-744238071f45",
   "metadata": {},
   "source": [
    "From this data we can simply train a model with an estimate (guess) of what the variance term should be and plot the results to see a near-perfect agrreement with the calucated values.\n",
    "\n",
    "These types of plots are called parity plots and are a commonly used tool for seeing how well a model fits the data. The closer all points are to the diagonal the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45789112-2206-409e-84c3-ae9703c95e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_process = GaussianProcessRegressor(\n",
    "    kernel=kernel, alpha=1.0, n_restarts_optimizer=9\n",
    ")\n",
    "\n",
    "gaussian_process.fit(df, volume)\n",
    "vol_prediction, vol_std = gaussian_process.predict(df, return_std=True)\n",
    "\n",
    "plt.plot(vol_prediction, volume, 'o')\n",
    "plt.ylabel(\"Calculated Volume ($\\\\AA^3$)\")\n",
    "plt.xlabel(\"Predicted Volume ($\\\\AA^3$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24473c-9810-4076-bf7c-d14e0f07a7d7",
   "metadata": {},
   "source": [
    "This result looks quite impressive, but in reality it is meaningless. The training error can approach 0 by simply fitting a linear model with $n$ random vectors, where $n$ is the number of data points in the training set. This model will perfectly fit all data in the training set, but not be able to predict any new point.\n",
    "\n",
    "To evaluate how well a model is doing we can perform a cross-validation where we split the data set into a training and validation sets and evaulate a model trained on a training set with the validation set. Here we will use 5-fold cross validation which splits the dataset into 5 training and validation sets where each data point in the dataset appears in exactly one validation set.\n",
    "\n",
    "While this will give a decent estimate of the \"true\" model error, the best way to do this is still an active area of research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff65d6-c666-439d-944a-1911c97edf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "k_fold = KFold(n_splits=5)\n",
    "vol_pred = np.zeros(len(volume))\n",
    "rmses = []\n",
    "for train_inds, test_inds in k_fold.split(df):\n",
    "    print(f\"Test inds: {test_inds}\")\n",
    "    \n",
    "    X_train = df.iloc[train_inds, :]\n",
    "    X_test = df.iloc[test_inds, :]\n",
    "    \n",
    "    y_train = volume.values[train_inds]\n",
    "    y_test = volume.values[test_inds]\n",
    "\n",
    "    gaussian_process.fit(X_train, y_train)\n",
    "    vol_pred[test_inds] = gaussian_process.predict(X_test)\n",
    "    \n",
    "    rmses.append(np.sqrt(np.mean((vol_pred[test_inds] - y_test) ** 2.0)))\n",
    "\n",
    "print(rmses)\n",
    "plt.plot(vol_pred, volume, 'o')\n",
    "plt.plot([10, 300], [10, 300])\n",
    "plt.ylabel(\"Calculated Volume ($\\\\AA^3$)\")\n",
    "plt.xlabel(\"Predicted Volume ($\\\\AA^3$)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5957396-3e90-4447-a4f5-a17b1aace340",
   "metadata": {},
   "source": [
    "We now see that our model is not really doing well with it having a large tendency to under-estimate the actually calculated values. This means that our model is over-fitting and tailoring the model to match the dataset. This is a common thing in machine learning, which is why cross-validation is paramount to do for any ML project.\n",
    "\n",
    "Now we want to see if we can improve this model by tuning the hyperparameters for the GPR model: The alpha parameter and the length scale of the kernel. This procedure is known as hyperparameter optimization and is setting the free parameters of a ML model that are not directly set from the training process. In the above example we made a guess of what they should be, and unsurprisingly that guess was bad, a systematic search for the optimal values must be done. \n",
    "\n",
    "There are multiple ways of tuning these parameters, but we will use the simplest method, a basic grid search. This is going to basically try a range of values to see what performs best, but other methods do this more efficently. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0fc139-93d6-4c1f-b964-b3ba48440d4d",
   "metadata": {},
   "source": [
    "1) Define Functions needed for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be35bd7d-2c38-4684-9b90-c13a0fb22dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, y, alpha, length_scale):\n",
    "    \"\"\"Train a gaussian process regression model\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): Training data\n",
    "        y (np.array[float]): Training labels\n",
    "        alpha (float): Gaussian Measurement noise\n",
    "        length_scale (float): Length scale of the kernel\n",
    "\n",
    "    Returns:\n",
    "        GaussianProcessRegressor: Trained model\n",
    "    \"\"\"\n",
    "    kernel = 1 * RBF(length_scale=length_scale, length_scale_bounds=(1e-12, 1e12))\n",
    "    \n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, alpha=alpha)\n",
    "    gpr.fit(X, y)\n",
    "    \n",
    "    return gpr\n",
    "\n",
    "def k_fold_cross_valiation(X, y, n_splits, alpha, ls):\n",
    "    \"\"\"Perform a 5-fold cross-validation on a data-set X, y\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): The training data\n",
    "        y (pd.Series): The training labels\n",
    "        n_splits (int): The number of splits to use\n",
    "        alpha (float): The alpha parameter for GPR\n",
    "        ls (float): Length scale for the RBF Kernel\n",
    "    \n",
    "    Returns:\n",
    "        tupe[np.ndarray[float], np.ndarray[float]]: The mean prediction and the std of the predictions\n",
    "    \"\"\"    \n",
    "    k_fold = KFold(n_splits=n_splits)\n",
    "\n",
    "    y_pred = np.zeros(len(y))\n",
    "    y_std = np.zeros(len(y))\n",
    "    for train_inds, test_inds in k_fold.split(X):\n",
    "        gpr = train_model(\n",
    "            X.iloc[train_inds, :], \n",
    "            y.iloc[train_inds],\n",
    "            alpha,\n",
    "            ls,\n",
    "        )\n",
    "        y_pred[test_inds], y_std[test_inds] = gpr.predict(X.iloc[test_inds, :], return_std=True)\n",
    "\n",
    "    return y_pred, y_std\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Caclulate the rmse of an approximation\n",
    "    Args:\n",
    "        y_true (np.array[float]): The True values\n",
    "        y_pred (np.array[float]): The predicted values\n",
    "\n",
    "    Returns:\n",
    "        float: The RMSE of the model\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4f70b-39aa-4a58-8433-4a5217523b47",
   "metadata": {},
   "source": [
    "2) Define grid of hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d8d31-d106-4257-b877-20800f9e36a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = 10.0 ** np.arange(-6, 3, 1)\n",
    "length_scale = 10.0 ** np.arange(-4, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fde3b0-716d-4d45-93b8-4d89d3ea3c16",
   "metadata": {},
   "source": [
    "Initialize The min values of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf8d5ba-0aaf-48d7-9928-8a44846d1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_rmse = np.inf # Min RMSE should be the highest possible value (np.inf, infinity)\n",
    "min_hp = ()\n",
    "min_preds = []\n",
    "min_stds = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa069980-c861-4077-acd3-76171fc11823",
   "metadata": {},
   "source": [
    "Loop over all combinations of alpha and ls in a grid search using iter tools product\n",
    "\n",
    "Itertools prodcut takes two lists and creates all combinations: Example\n",
    "\n",
    "```\n",
    "lst_1 = [A, B, C, D]\n",
    "lst_2 = [1, 2]\n",
    "product(lst_1, lst_2) = [(A, 1), (A, 2), (B, 1), (B, 2), (C, 1), (C, 2), (D, 1), (D, 2)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d04b9b9-c12d-4450-ac1f-5c363580f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "for alpha, ls in product(alphas, length_scale):\n",
    "    vol_pred, vol_std = k_fold_cross_valiation(df, volume, 5, alpha, ls)\n",
    "    \n",
    "    rmse_cur = rmse(vol_pred, volume.values)\n",
    "    if rmse_cur < min_rmse:\n",
    "        print(f\"alpha: {alpha}, ls: {ls}, rmse: {rmse_cur}, \")\n",
    "        min_rmse = rmse_cur\n",
    "        min_hp = (alpha, ls)\n",
    "        min_preds = vol_pred\n",
    "        min_std = vol_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a1f8b-a770-4095-b09f-0e7c2c98741a",
   "metadata": {},
   "source": [
    "We can now see how well this model is performing by plotting the validation error from the cross-validation examples.\n",
    "\n",
    "To make it easier lets define a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccfc58-3810-4766-bf3c-184a3d22134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def plot_parity_plot(y_true, y_pred, y_std):\n",
    "    \"\"\"Plots a parity plot of a model\n",
    "\n",
    "    Args:\n",
    "        y_true (np.ndarray[float]): The True values of the model\n",
    "        y_pred (np.ndarray[float]): The predicted values\n",
    "        y_std (np.array[float]): The standard deviation of the model\n",
    "\n",
    "    Returns:\n",
    "        mpl.Figure, mpl.Axes: The figure and axes of the parity plot\n",
    "    \"\"\"\n",
    "    R = pearsonr(y_true, y_pred).statistic\n",
    "    rmse_val = rmse(y_true, y_pred)\n",
    "    \n",
    "    min_val = min(np.min(y_true), np.min(y_pred))\n",
    "    max_val = max(np.max(y_true), np.max(y_pred))\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    # Plot the parity line as a gray dashed line\n",
    "    ax.plot([min_val - 10, max_val + 10], [min_val - 10, max_val + 10], \"--\", c=\"gray\")\n",
    "    plt.errorbar(\n",
    "        y_true,\n",
    "        y_pred,\n",
    "        y_std,\n",
    "        linestyle=\"None\",\n",
    "        color=\"tab:blue\",\n",
    "        marker=\".\",\n",
    "        markersize=10,\n",
    "        label=\"Observations\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Calculated Volume ($\\\\AA^3$)\")\n",
    "    ax.set_ylabel(\"Predicted Volume ($\\\\AA^3$)\")\n",
    "    \n",
    "    ax.set_xlim([0, 310])\n",
    "    ax.set_ylim([0, 310])\n",
    "    \n",
    "    ax.text(10, 295, f\"Validation RMSE: {rmse_val:0.1f}\")\n",
    "    ax.text(10, 275, f\"Validation R$^2$: {R ** 2.0:0.2f}\")\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80da73f6-1bb8-442d-bc8d-1bb6a7e7d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_parity_plot(volume, min_preds, min_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e133b16-6317-4167-a316-7a4e77e2a4f7",
   "metadata": {},
   "source": [
    "**Problem 3**\n",
    "\n",
    "Create a hyperparameter optimization function for alpha and ls and use it to see how changing the number of splits affects the validation results.\n",
    "\n",
    "Things to think about\n",
    "\n",
    "1) What are our inputs?\n",
    "2) What is the output?\n",
    "3) What code should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a119c24-d791-4f1a-b9e5-c5bfa9fcdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyper_param_opt(X, y, n_splits, alphas=None, lengh_scales=None):\n",
    "    \"\"\"Optimize the hpyerparameters for a given dataset\n",
    "\n",
    "    Args:\n",
    "        X (pd.DataFrame): training data\n",
    "        y (pd.Series): Training labels\n",
    "        n_splits (int): number of splits for k-fold cross_validation\n",
    "        alphas (Optional[np.ndarray[float]]): List of all alpha values to test\n",
    "        lengh_scales (Optional[np.ndarray[float]]): List of all length_scale values to test\n",
    "\n",
    "    Returns:\n",
    "        tuple[int, int]: The optimized alpha and length scale\n",
    "    \"\"\"\n",
    "    if alphas is None:\n",
    "        alphas = 10.0 ** np.arange(-4, 3, 1)\n",
    "    \n",
    "    if lengh_scales is None:\n",
    "        length_scales = 10.0 ** np.arange(-4, 3, 1)\n",
    "\n",
    "    min_hp = ()\n",
    "    min_rmse = 1e10\n",
    "    \n",
    "    for alpha, ls in product(alphas, length_scales):\n",
    "        y_pred, _ = k_fold_cross_valiation(X, y, n_splits, alpha, ls)\n",
    "        rmse_cur = rmse(y.values, y_pred)\n",
    "        if rmse_cur < min_rmse:\n",
    "            min_rmse = rmse_cur\n",
    "            min_hp = (alpha, ls)\n",
    "    return min_hp\n",
    "\n",
    "alpha, ls = hyper_param_opt(df, volume, 2)\n",
    "vol_pred, vol_std = k_fold_cross_valiation(df, volume, 2, alpha, ls)\n",
    "fig_2, ax_2 = plot_parity_plot(volume, vol_pred, vol_std)\n",
    "ax_2.text(10, 255, f\"n_splits: 2\")\n",
    "\n",
    "alpha, ls = hyper_param_opt(df, volume, 10)\n",
    "vol_pred, vol_std = k_fold_cross_valiation(df, volume, 10, alpha, ls)\n",
    "fig_10, ax_10 = plot_parity_plot(volume, vol_pred, vol_std)\n",
    "ax_10.text(10, 255, f\"n_splits: 10\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ff7cf3-fb52-4425-a824-44ac77a6ca22",
   "metadata": {},
   "source": [
    "As we can see from the answers of the last problem, in general the smaller your training set is relative to the overall dataset size, the larger the validation errors are for the trained models. We can check how our models are doing by generating the learning curves and comparing the training and test RMSEs with different sizes of our training data. Please note that better hyper-parameter optimization would be need to fully trust these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0e5ad-a3bd-4a23-b1a7-e3022bfb7cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "    \n",
    "indexes = np.arange(len(volume), dtype=np.int64)\n",
    "rng.shuffle(indexes)\n",
    "\n",
    "X_shuffle = df.iloc[indexes, :]\n",
    "y_shuffle = volume.iloc[indexes]\n",
    "\n",
    "n_update = 15\n",
    "n_train = n_update\n",
    "\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "\n",
    "while n_train < len(indexes):\n",
    "    X_train = X_shuffle.iloc[:n_train, :]\n",
    "    X_test = X_shuffle.iloc[n_train:, :]\n",
    "\n",
    "    y_train = y_shuffle.iloc[:n_train]\n",
    "    y_test = y_shuffle.iloc[n_train:]\n",
    "    \n",
    "    alpha, ls = hyper_param_opt(X_train, y_train, min(n_update, 5))\n",
    "    print(n_train, alpha, ls)\n",
    "    gpr = train_model(X_train, y_train, alpha, ls)\n",
    "    \n",
    "    train_rmse.append(rmse(y_train, gpr.predict(X_train)))\n",
    "    test_rmse.append(rmse(y_test, gpr.predict(X_test)))\n",
    "    \n",
    "    n_train += n_update\n",
    "\n",
    "plt.plot(range(n_update, len(indexes), n_update), train_rmse)\n",
    "plt.plot(range(n_update, len(indexes), n_update), test_rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c3549-98c5-45d8-ba75-e201f8caee8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
